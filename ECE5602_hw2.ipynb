{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment1. Softmax and 2-layer Neural Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "import time\n",
    "\n",
    "# set default plot options\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up input preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to download the data from the CIFAR-10 website. To do this, simply run following command, and then load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'.' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input dataset.\n",
    "!./get_cifar10.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load all of CIFAR10 dataset.\n",
    "def load_CIFAR10(root):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(root, 'data_batch_%d' % (b, ))\n",
    "        with open(f, 'rb') as f:\n",
    "            datadict = pickle.load(f)\n",
    "            X = datadict['data']\n",
    "            Y = datadict['labels']\n",
    "            X = X.reshape(10000,3,32,32).transpose(0,2,3,1).astype(\"float\")\n",
    "            Y = np.array(Y)\n",
    "        #X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X,Y\n",
    "    \n",
    "    f=os.path.join(root, 'test_batch')\n",
    "    with open(f, 'rb') as f:\n",
    "        datadict = pickle.load(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        Xte = X.reshape(10000,3,32,32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Yte = np.array(Y)\n",
    "        \n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './cifar-10-batches-py\\\\data_batch_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a706c665187b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_CIFAR10_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Train data shape : %s,  Train labels shape : %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Validatoin data shape : %s,  Validation labels shape : %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a706c665187b>\u001b[0m in \u001b[0;36mget_CIFAR10_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_CIFAR10_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[1;31m# 1. Load the raw data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_CIFAR10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./cifar-10-batches-py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;31m# 2. Divide the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-766654351872>\u001b[0m in \u001b[0;36mload_CIFAR10\u001b[0;34m(root)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data_batch_%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mdatadict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './cifar-10-batches-py\\\\data_batch_1'"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data():\n",
    "    # 1. Load the raw data\n",
    "    X_tr, Y_tr, X_te, Y_te = load_CIFAR10('./cifar-10-batches-py')\n",
    "    \n",
    "    # 2. Divide the data\n",
    "    X_val, Y_val = X_tr[49000:], Y_tr[49000:]\n",
    "    X_tr, Y_tr = X_tr[:49000], Y_tr[:49000]\n",
    "    X_te, Y_te = X_te[:1000], Y_te[:1000]\n",
    "\n",
    "    # 3. Preprocess the input image\n",
    "    X_tr = np.reshape(X_tr, (X_tr.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0],-1))\n",
    "    X_te = np.reshape(X_te, (X_te.shape[0],-1))\n",
    "    \n",
    "    # 4. Normalize the data (subtract the mean image)\n",
    "    mean_img = np.mean(X_tr, axis=0)\n",
    "    X_tr -= mean_img\n",
    "    X_val -= mean_img\n",
    "    X_te -= mean_img\n",
    "    \n",
    "    # 5. Add bias and Transform into columns\n",
    "    X_tr = np.hstack([X_tr, np.ones((X_tr.shape[0],1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0],1))])\n",
    "    X_te = np.hstack([X_te, np.ones((X_te.shape[0],1))])\n",
    "    \n",
    "    return X_tr, Y_tr, X_val, Y_val, X_te, Y_te\n",
    "\n",
    "\n",
    "\n",
    "X_tr, Y_tr, X_val, Y_val, X_te, Y_te = get_CIFAR10_data()\n",
    "print 'Train data shape : %s,  Train labels shape : %s' % (X_tr.shape, Y_tr.shape)\n",
    "print 'Validatoin data shape : %s,  Validation labels shape : %s' % (X_val.shape, Y_val.shape)\n",
    "print 'Test data shape : %s,  Test labels shape : %s' % (X_te.shape, Y_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement two version of loss functions for softmax classifier, and test it out on the CIFAR10 dataset.\n",
    "\n",
    "First, implement the naive softmax loss function with nested loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_loss_naive(W,X,Y,reg):\n",
    "    \"\"\"\n",
    "     Inputs have D dimension, there are C classes, and we operate on minibatches of N examples.\n",
    "    \n",
    "     Inputs :\n",
    "         - W : A numpy array of shape (D,C) containing weights.\n",
    "         - X : A numpy array of shape (N,D) contatining a minibatch of data.\n",
    "         - Y : A numpy array of shape (N,) containing training labels; \n",
    "               Y[i]=c means that X[i] has label c, where 0<=c<C.\n",
    "         - reg : Regularization strength. (float)\n",
    "         \n",
    "     Returns :\n",
    "         - loss as single float\n",
    "         - gradient with respect to weights W; an array of sample shape as W\n",
    "     \"\"\"\n",
    "    \n",
    "    # Initialize the loss and gradient to zero\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    \n",
    "    ####################################################################################################\n",
    "    # TODO : Compute the softmax loss and its gradient using explicit loops.                           # \n",
    "    #        Store the loss in loss and the gradient in dW.                                            #\n",
    "    #        If you are not careful here, it is easy to run into numeric instability.                  #\n",
    "    #        Don't forget the regularization.                                                          #\n",
    "    #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    for i in xrange(num_train):\n",
    "        scores = X[i].dot(W) # this is the prediction of training sample i, for each class\n",
    "        scores -= np.max(scores)\n",
    "        # calculate the probabilities that the sample belongs to each class\n",
    "        probabilities = np.exp(scores) / np.sum(np.exp(scores))\n",
    "        # loss is the log of the probability of the correct class\n",
    "        loss += -np.log(probabilities[y[i]])\n",
    "\n",
    "        probabilities[y[i]] -= 1 # calculate p-1 and later we'll put the negative back\n",
    "    \n",
    "        # dW is adjusted by each row being the X[i] pixel values by the probability vector\n",
    "        for j in xrange(num_classes):\n",
    "            dW[:,j] += X[i,:] * probabilities[j]\n",
    "\n",
    "    # Right now the loss is a sum over all training examples, but we want it\n",
    "    # to be an average instead so we divide by num_train.\n",
    "    loss /= num_train\n",
    "    dW /= num_train\n",
    "\n",
    "    # Add regularization to the loss.\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "    dW += reg * W\n",
    "    \n",
    "    #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "    ####################################################################################################\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random softmax weight matrix and use it to compute the loss. As a rough sanity check, our loss should be something close to -log(0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2d6f604999e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3073\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_loss_naive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'loss :'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'sanity check : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_tr' is not defined"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_tr, Y_tr, 0.0)\n",
    "\n",
    "print 'loss :', loss\n",
    "print 'sanity check : ', -np.log(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing is the vectorized softmax loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_loss_vectorized(W, X, Y, reg):\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    ####################################################################################################\n",
    "    # TODO : Compute the softmax loss and its gradient using no explicit loops.                        # \n",
    "    #        Store the loss in loss and the gradient in dW.                                            #\n",
    "    #        If you are not careful here, it is easy to run into numeric instability.                  #\n",
    "    #        Don't forget the regularization.                                                          #\n",
    "    #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "    num_train = X.shape[0]\n",
    "    scores = X.dot(W) # NxD * DxC = NxC\n",
    "    scores -= np.max(scores,axis=1,keepdims=True)\n",
    "    probabilities = np.exp(scores) / np.sum(np.exp(scores),axis=1,keepdims=True)\n",
    "    correct_class_probabilities = probabilities[range(num_train),y]\n",
    "\n",
    "    loss = np.sum(-np.log(correct_class_probabilities)) / num_train\n",
    "    # that was supposed to summarize across classes that aren't classified correctly\n",
    "    # so now we need to subtract 1 class for each case (a total of N) that are correctly classified\n",
    "    loss += 0.5 * reg * np.sum(W*W) \n",
    "\n",
    "    probabilities[range(num_train),y] -= 1\n",
    "    dW = X.T.dot(probabilities) / num_train\n",
    "    \n",
    "    #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "    ####################################################################################################\n",
    "    \n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare two versions. The two versions should compute the same results, but the vectorized version should be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-858494999d6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss_naive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_naive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_loss_naive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.00001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'naive loss : %e with %fs'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss_naive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ms_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ms_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_tr' is not defined"
     ]
    }
   ],
   "source": [
    "s_time = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_tr, Y_tr, 0.00001)\n",
    "print 'naive loss : %e with %fs' % (loss_naive, time.time()-s_time)\n",
    "\n",
    "s_time = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_tr, Y_tr, 0.00001)\n",
    "print 'vectorized loss : %e with %fs' % (loss_vectorized, time.time()-s_time)\n",
    "\n",
    "print 'loss difference : %f' % np.abs(loss_naive - loss_vectorized)\n",
    "print 'gradient difference : %f' % np.linalg.norm(grad_naive-grad_vectorized, ord='fro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should implement the softmax classifier using the comment below with softmax loss function you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Softmax(object):\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "        \n",
    "    def train(self, X, Y, learning_rate=1e-3, reg=1e-5, num_iters=100, batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this Softmax classifier using stochastic gradient descent.\n",
    "        \n",
    "        Inputs have D dimensions, and we operate on N examples.\n",
    "        \n",
    "        Inputs :\n",
    "            - X : A numpy array of shape (N,D) containing training data.\n",
    "            - Y : A numpy array of shape (N,) containing training labels;\n",
    "                  Y[i]=c means that X[i] has label 0<=c<C for C classes.\n",
    "            - learning_rate : (float) Learning rate for optimization.\n",
    "            - reg : (float) Regularization strength. \n",
    "            - num_iters : (integer) Number of steps to take when optimizing. \n",
    "            - batch_size : (integer) Number of training examples to use at each step.\n",
    "            - verbose : (boolean) If true, print progress during optimization.\n",
    "        \n",
    "        Regurns :\n",
    "            - A list containing the value of the loss function at each training iteration.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_train, dim = X.shape\n",
    "        num_classes = np.max(Y)+1\n",
    "        if self.W is None :\n",
    "            self.W = 0.001*np.random.randn(dim, num_classes)\n",
    "            \n",
    "        loss_history = []\n",
    "        for it in xrange(num_iters):\n",
    "            X_batch = None\n",
    "            Y_batch = None\n",
    "            \n",
    "        ####################################################################################################\n",
    "        # TODO : Sample batch_size elements from the training data and their corresponding labels          #\n",
    "        #        to use in this round of gradient descent.                                                 #\n",
    "        #        Store the data in X_batch and their corresponding labels in Y_batch; After sampling       #\n",
    "        #        X_batch should have shape (dim, batch_size) and Y_batch should have shape (batch_siae,)   #\n",
    "        #                                                                                                  #\n",
    "        #        Hint : Use np.random.choice to generate indicies.                                         #\n",
    "        #               Sampling with replacement is faster than sampling without replacement.             #\n",
    "        #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "        rand_idx = np.random.choice(num_train, batch_size)\n",
    "        X_batch = X[:, rand_idx]\n",
    "        y_batch = y[rand_idx]\n",
    "        \n",
    "        #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "        ####################################################################################################\n",
    "\n",
    "        # Evaluate loss and gradient\n",
    "        loss, grad = self.loss(X_batch, Y_batch, reg)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Perform parameter update\n",
    "        ####################################################################################################\n",
    "        # TODO : Update the weights using the gradient and the learning rate                               #\n",
    "        #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "        self.W += -1 * learning_rate * grad\n",
    "        #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "        ####################################################################################################\n",
    "        \n",
    "            if verbose and it % num_iters == 0:\n",
    "                print 'Ieration %d / %d : loss %f ' % (it, num_iters, loss)\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this softmax classifier to predict labels for data points.\n",
    "        \n",
    "        Inputs :\n",
    "            - X : A numpy array of shape (N,D) containing training data.\n",
    "            \n",
    "        Returns :\n",
    "             - Y_pred : Predicted labels for the data in X. Y_pred is a 1-dimensional array of length N, \n",
    "                        and each element is an integer giving the predicted class.\n",
    "        \"\"\"\n",
    "        Y_pred = np.zeros(X.shape[0])\n",
    "        \n",
    "        ####################################################################################################\n",
    "        # TODO : Implement this method. Store the predicted labels in Y_pred                               #\n",
    "        #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "        scores = np.dot(self.W, X)\n",
    "        y_pred = scores.argmax(axis=0)\n",
    "        #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "        ####################################################################################################\n",
    "        return Y_pred\n",
    "    \n",
    "    \n",
    "    def loss(self, X_batch, Y_batch, reg):\n",
    "        return softmax_loss_vectorized(self.W, X_batch, Y_batch, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the validatoin set to tune hyperparemeters (regularizatoin strength and learning rate).\n",
    "You should experiment with different range for the learning rates and regularization strength;\n",
    "if you are careful you should be able to get a classification accuracy of over 0.35 on the validatoin set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validatoin accuracy achieved during cross-validation : -1\n"
     ]
    }
   ],
   "source": [
    "# results is dictionary mapping tuples of the form.\n",
    "# (learning_rate, regularization_strength) to tuple of the form (training_accuracy, validation_accuracy).\n",
    "# The accuracy is simply the fraction of data points that are correctly classified.\n",
    "softmax = Softmax()\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [5e-5, 8e-6, 1e4]\n",
    "regularization_strengths = [2e-4, 6e-2]\n",
    "\n",
    "#########################################################################################################\n",
    "# TODO : Write code that chooses the best hyperparameters by tuning on the validation set.              # \n",
    "#        For each combination of hyperparemeters, train a Softmax on the training set,                  #\n",
    "#        compute its accuracy on the training and validatoin sets, and store these numbers in the       #\n",
    "#        results dictionary. In addition, store the best validation accuracy in best_val                #\n",
    "#        and the Softmax object that achieves this accuracy in best_softmax.                            #\n",
    "#                                                                                                       #\n",
    "# Hint : You should use a small value for num_iters as you develop your validation code so that the     #\n",
    "#        Softmax don't take much time to train; once you are confident that your validation code works, #\n",
    "#        you should rerun the validation code with a larger value for num_iter.                         #\n",
    "#------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "iters = 100\n",
    "for lr in learning_rates:\n",
    "    for rs in regularization_strengths:\n",
    "        softmax = Softmax()\n",
    "        softmax.train(X_train, y_train, learning_rate=lr, reg=rs, num_iters=iters)\n",
    "        \n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        acc_train = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        acc_val = np.mean(y_val == y_val_pred)\n",
    "        \n",
    "        results[(lr, rs)] = (acc_train, acc_val)\n",
    "        \n",
    "        if best_val < acc_val:\n",
    "            best_val = acc_val\n",
    "            best_softmax = softmax\n",
    "\n",
    "#-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "#########################################################################################################\n",
    "\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy : %f, val accuracy : %f ' % (lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validatoin accuracy achieved during cross-validation :', best_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the best softmax on testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-86bcf5b62d8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY_te_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_softmax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_te\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mY_te_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'softmax on raw pixels final test set accuracy : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "Y_te_pred = best_softmax.predict(X_te)\n",
    "test_accuracy = np.mean(Y_te == Y_te_pred)\n",
    "\n",
    "print 'softmax on raw pixels final test set accuracy : ', test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the learned weights for each class. Depending on your choice of learning rate and regularization strength, these may or may not be nice to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'W'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-445f256d83c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_softmax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mw_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'W'"
     ]
    }
   ],
   "source": [
    "w = best_softmax.W[:-1, :]\n",
    "w = w.reshape(32,32,3,10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    \n",
    "    wimg=255.0*(w[:,:,:,i].squeeze() - w_min)/(w_max-w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Two layer Neural Network classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will develop a neural network with fully-connected layers to perform classification, and test it out on the CIFAR10 dataset. We will use the class TwoLayerNet to present instances of our network. The network parameters are stored in the instance variable self.params where keys are string parameter names and values are numpy arrays.\n",
    "\n",
    "Implement all functions in TwoLayerNet class in the sequence listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    \"\"\"\n",
    "    A two-layer fully-connected neural network. The net has an input dimension of N,\n",
    "    a hidden layer dimension of H, and performs clasisfication over C classes.\n",
    "    We train the network with a softmax loss function and L2 regularization on the weight matrices.\n",
    "    The network uses a ReLU nonlinearity after the first fully connected layer.\n",
    "    \n",
    "    In other words, the network has the following architecture :\n",
    "        input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "    \n",
    "    The outputs of the second fully-connected layer are the scores for each class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        \"\"\"\n",
    "        Initialize the model. Weights are initialized to small random values and biases are \n",
    "        initialized to zero. Weights and biases are stored in the variable self.params, which is \n",
    "        a dictionary with the following keys:\n",
    "        \n",
    "            W1 : First layer weights ; has shape (D,H)\n",
    "            b1 : First layer biases ; has shape (H,)\n",
    "            W2 : Second layer weights ; has shape (H,C)\n",
    "            b2 : Second layer biases ; has shape (C,)\n",
    "\n",
    "        Inputs:\n",
    "            - input_size : The dimension D of the input data.\n",
    "            - hidden_size : The number of neurons H in the hidden layer.\n",
    "            - output_size : The number of classes C.\n",
    "        \"\"\"\n",
    "        self.params={}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def loss(self, X, Y=None, reg=0.0):\n",
    "        \"\"\"\n",
    "        Compute the loss and gradients for a two layer fully connected neural network.\n",
    "        \n",
    "        Inputs :\n",
    "            - X : Input data of shape (N,D). Each X[i] is a training sample.\n",
    "            - Y : Vector of training labels. Y[i] is the label for X[i], and each Y[i] is an integer\n",
    "                  in the range 0<=Y[i]<C. This parameter is optional; If it is not passed then \n",
    "                  we only return scores, and if it is passed then we instead return the loss and gradiens. \n",
    "            - reg : Regularization strength.\n",
    "            \n",
    "        Returns :\n",
    "            - If Y is None, return a matrix scores of shape (N,C) where scores [i,c] is the score \n",
    "              for the class c on input X[i].\n",
    "            - If Y is not None, instead return a tuple of :\n",
    "                * loss : Loss (data loss and regularization loss) for this batch of training samples.\n",
    "                * grads : Dictionary mapping parameter names to gradients of those parameters \n",
    "                          with respect to the loss function ; has the same keys as self.params.\n",
    "        \"\"\"\n",
    "        #Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "        \n",
    "        #Compute the scores\n",
    "        scores = None\n",
    "        ####################################################################################################\n",
    "        # TODO: Perform the forward pass, computing the class scores for the input. Store the result in    #\n",
    "        # in the scores variable, which should be an array of shape (N, C).                                #\n",
    "        #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "        z1 = np.dot(X,W1) + b1\n",
    "        H1 = np.maximum(0, z1)\n",
    "        scores = np.dot(H1,W2) + b2\n",
    "        #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "        ####################################################################################################\n",
    "        \n",
    "        \n",
    "        #If the targets are not given then jump out, we're done.\n",
    "        if Y is None :\n",
    "            return scores\n",
    "        \n",
    "        \n",
    "        #Compute the loss\n",
    "        loss = None\n",
    "        ####################################################################################################\n",
    "        # TODO : Finish the forward pass, and compute the loss. This should include both the data loss and #\n",
    "        # L2 regularization for W1 and W2. Store the result in the variable loss, which should be a scalar.#\n",
    "        # Use the Softmax classifier loss. So that your results match ours, multiply the regularization    #\n",
    "        # loss by 0.5.                                                                                     #\n",
    "        #                                                                                                  #\n",
    "        # Hint : This function is pretty much the same when you compute the softmax loss. The only         #\n",
    "        # difference is that we use L2 regularization for W1 and W2.                                       # \n",
    "        #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "        num_train = X.shape[0]\n",
    "        scores -= np.max(scores,axis=1,keepdims=True)\n",
    "        probabilities = np.exp(scores) / np.sum(np.exp(scores),axis=1,keepdims=True)\n",
    "        correct_class_probabilities = probabilities[range(num_train),y]\n",
    "\n",
    "        loss = np.sum(-np.log(correct_class_probabilities)) / num_train\n",
    "        # that was supposed to summarize across classes that aren't classified correctly\n",
    "        # so now we need to subtract 1 class for each case (a total of N) that are correctly classified\n",
    "        loss += 0.5 * reg * (np.sum(W1*W1) + np.sum(W2*W2))\n",
    "        # print 'loss=',loss\n",
    "        #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "        ####################################################################################################\n",
    "        \n",
    "        \n",
    "        #Backward pass : compute gradinets\n",
    "        grads = {}\n",
    "        ####################################################################################################\n",
    "        # TODO: Compute the backward pass, computing the derivatives of the weights and biases.            #\n",
    "        # Store the results in the grads dictionary. For example, grads['W1'] should store the gradient    #\n",
    "        # on W1, and be a matrix of same size.                                                             #    \n",
    "        #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "        probabilities[range(num_train),y] -= 1\n",
    "        probabilities /= num_train\n",
    "        # backpropagate, the output of the middle layer should be the weight times probabilities (output layer)\n",
    "        out1 = probabilities.dot(W2.T) \n",
    "        # reverse the max() gate\n",
    "        out1[z1 <= 0] = 0\n",
    "        grads['W2'] = np.dot(H1.T,probabilities) + reg * W2\n",
    "        grads['W1'] = np.dot(X.T, out1) + reg * W1\n",
    "        grads['b2'] = np.sum(probabilities, axis=0)\n",
    "        grads['b1'] = np.sum(out1, axis=0)\n",
    "\n",
    "        #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "        ####################################################################################################\n",
    "        \n",
    "        return loss, grads\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, X, Y, X_val, Y_val, learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "              reg=1e-5, num_iters=100, batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this neural network using stochastic gradient descent.\n",
    "        \n",
    "        Inputs :\n",
    "            - X : A numpy array of shape (N,D) giving training data.\n",
    "            - Y : A numpy array of shape(N,) giving training labels; Y[i]=c means that X[i] has label c, \n",
    "                  where 0<=c<C.\n",
    "            - X_val : A numpy array of shape (N_val, D) giving validation data.\n",
    "            - Y_val : A numpy array of shape (N_val,) giving validation labels.\n",
    "            - learning_rate : Scalar giving learning rate for optimization.\n",
    "            - learning_rate_decay : Scalar giving factor used to decay the learning rate.\n",
    "            - reg : Scalar giving regularization strength.\n",
    "            - num_iters : Number of steps to take when optimizing.\n",
    "            - batch_size : Number of training examples to use per step.\n",
    "            - verbose : boolean; if true print progress during optimization.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train/batch_size,1)\n",
    "        \n",
    "        #Use SGD to optimize the parameter in self.model\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "        \n",
    "        for it in xrange(num_iters):\n",
    "            X_batch = None\n",
    "            Y_batch = None\n",
    "            \n",
    "            ################################################################################################\n",
    "            # TODO: Create a random minibatch of training data and labels, storing then in X_batch and     #\n",
    "            # Y_batch respectively.                                                                        #\n",
    "            #-------------------------------------WRITE YOUR CODE------------------------------------------#\n",
    "            batch_idxs = np.random.choice(num_train, batch_size, replace=True)\n",
    "\n",
    "            X_batch = X[batch_idxs,:]\n",
    "            y_batch = y[batch_idxs]\n",
    "            #------------------------------------END OF YOUR CODE------------------------------------------#\n",
    "            ################################################################################################\n",
    "            \n",
    "            \n",
    "            #Compute loss and gradients using the current minibatch\n",
    "            loss, grads = self.loss(X_batch, Y=Y_batch, reg=reg)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            \n",
    "            ################################################################################################\n",
    "            # TODO: Use the gradients in the grads dictionary to update the parameters of the network      #\n",
    "            # (stored in the dictionary self.params) using stochastic gradient descent. You'll need to use #\n",
    "            # the gradients stored in the grads dictionary defined above.                                  #\n",
    "            #-------------------------------------WRITE YOUR CODE------------------------------------------#\n",
    "            self.params['W1'] -= learning_rate * grads['W1']\n",
    "            self.params['W2'] -= learning_rate * grads['W2']\n",
    "            self.params['b1'] -= learning_rate * grads['b1']\n",
    "            self.params['b2'] -= learning_rate * grads['b2']\n",
    "            #------------------------------------END OF YOUR CODE------------------------------------------#\n",
    "            ################################################################################################\n",
    "            \n",
    "            if verbose and it % 100 == 0:\n",
    "                print 'Iteration %d / %d : loss %f ' % (it, num_iters, loss)\n",
    "                \n",
    "            #Every epoch, check train and val accuracy and decay learning rate.\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                #Check accuracy\n",
    "                #train_acc = (self.predict(X_batch)==Y_batch).mean()\n",
    "                #val_acc = (self.predict(X_val)==Y_val).mean()\n",
    "                train_acc = np.mean(self.predict(X_batch)==Y_batch)\n",
    "                val_acc = np.mean(self.predict(X_val)==Y_val)\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "                \n",
    "                #Decay learning rate\n",
    "                learning_rate *= learning_rate_decay\n",
    "                \n",
    "        return {\n",
    "            'loss_history' : loss_history,\n",
    "            'train_acc_history' : train_acc_history,\n",
    "            'val_acc_history' : val_acc_history,\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this two-layer network to predict labels for data points.\n",
    "        For each data point we predict scores for each of the C classes, and assign each data point\n",
    "        to the class with the highest score.\n",
    "        \n",
    "        Inputs :\n",
    "            - X : A numpy array of shape (N,D) giving N,D-dimensional data points to classify.\n",
    "            \n",
    "        Returns :\n",
    "            - Y_pred : A numpy array of shape (N,) giving predicted lables for each of the elements of X.\n",
    "                       For all i, Y_pred[i] = c mean that X[i] is predicted to have class c, where 0<=c<C.\n",
    "        \"\"\"\n",
    "        \n",
    "        Y_pred = None\n",
    "        ################################################################################################\n",
    "        # TODO: Implement this function; it should be VERY simple!                                     #\n",
    "        #-------------------------------------WRITE YOUR CODE------------------------------------------#\n",
    "        H1 = np.maximum(0,X.dot(self.params['W1']) + self.params['b1'])\n",
    "        out = np.maximum(0,H1.dot(self.params['W2'])+self.params['b2'])\n",
    "        y_pred = np.argmax(out,axis=1)\n",
    "        \n",
    "        #------------------------------------END OF YOUR CODE------------------------------------------#\n",
    "        ################################################################################################\n",
    "\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize toy data and a toy model that we will use to develop your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    Y = np.array([0,1,2,2,1])\n",
    "    return X, Y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, Y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass : compute scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should implement the first part of TwoLayerNet.loss(). This function is very similar to the loss functions you have written for Softmax : It takes the data and weights and computes the class scores, the loss, and the gradients on the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "None\n",
      "\n",
      "correct scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "Difference between your scores and correct scores:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a468a54f4b9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;31m# The difference should be very small. We get < 1e-7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Difference between your scores and correct scores:'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[1;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcorrect_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "scores = net.loss(X)\n",
    "print 'Your scores:'\n",
    "print scores\n",
    "print\n",
    "print 'correct scores:'\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print correct_scores\n",
    "print\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print 'Difference between your scores and correct scores:'\n",
    "print np.sum(np.abs(scores - correct_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass : compute loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same function, implement the second part that computes the data and regularizatoin loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct loss:  1.30378789133\n",
      "Computed loss :  None\n",
      "Difference between your loss and correct loss:"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-03c5afc23f9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Correct loss: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Computed loss : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Difference between your loss and correct loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mcorrect_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "loss, _ = net.loss(X, Y, reg=0.1)\n",
    "correct_loss = 1.30378789133\n",
    "\n",
    "# should be very small, we get < 1e-12\n",
    "print 'Correct loss: ', correct_loss\n",
    "print 'Computed loss : ', loss\n",
    "print 'Difference between your loss and correct loss:', np.sum(np.abs(loss-correct_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass : compute gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the rest of the TwoLayerNet.loss function. This will compute the gradient of the loss with respect to the variables W1, b1, W2, and b2. Now that you have a correctly implemented forward pass, you can debug your backward pass using a numeric gradient check : If your implementatoin is correct, the difference between the numeric and analytic gradients should be less than 1e-8 for each of W1, b1, W2, and b2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "    \"\"\" \n",
    "        a naive implementaion of numerical gradient of f at x.\n",
    "        - f should be a function that takes a single argument.\n",
    "        - x is the point (numpy array) to evaluate the gradient at.\n",
    "    \"\"\" \n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print ix, grad[ix]\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss, grads = net.loss(X, Y, reg=0.1)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, Y, reg=0.1)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print '%s max relative error : %e ' % (param_name, rel_error(param_grad_num, grads[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network we will use stochastic gradient descent(SGD), similar to the Softmax classifier. Look at the function TwoLayerNet.train() and fill in the missing sections to implement the training procedure. This should be very similar to the training procedure you used for the Softmax classifier. \n",
    "\n",
    "You will also have to implement TwoLayerNet.predict(), as the training process periodically performs prediction to keep track of accuracy over time while the network trains.\n",
    "\n",
    "Once you have implemented the method, run the code below to train a two-layer network on toy data. You should achieve a training loss less than 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-448e95d2c984>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_toy_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Final training loss :'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss_history'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d4242d8cd5a3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, X_val, Y_val, learning_rate, learning_rate_decay, reg, num_iters, batch_size, verbose)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[1;31m#Compute loss and gradients using the current minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mloss_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d4242d8cd5a3>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, Y, reg)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[1;31m#Compute the scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "net = init_toy_model()\n",
    "stats = net.train(X,Y,X,Y,learning_rate=1e-1, reg=1e-5, num_iters=100, verbose=False)\n",
    "\n",
    "print 'Final training loss :', stats['loss_history'][-1]\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have implemented a two-layer network that passes gradient checks and works on toy data, it's time to load up our CIFAR10 data so we can use it to train a classifier on a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './cifar-10-batches-py\\\\data_batch_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-dc13cbfdc3cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_CIFAR10_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Train data shape : %s,  Train labels shape : %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Validatoin data shape : %s,  Validation labels shape : %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-dc13cbfdc3cd>\u001b[0m in \u001b[0;36mget_CIFAR10_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_CIFAR10_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[1;31m# 1. Load the raw data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_CIFAR10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./cifar-10-batches-py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;31m# 2. Divide the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-766654351872>\u001b[0m in \u001b[0;36mload_CIFAR10\u001b[0;34m(root)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data_batch_%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mdatadict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './cifar-10-batches-py\\\\data_batch_1'"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data():\n",
    "    # 1. Load the raw data\n",
    "    X_tr, Y_tr, X_te, Y_te = load_CIFAR10('./cifar-10-batches-py')\n",
    "    \n",
    "    # 2. Divide the data\n",
    "    X_val, Y_val = X_tr[49000:], Y_tr[49000:]\n",
    "    X_tr, Y_tr = X_tr[:49000], Y_tr[:49000]\n",
    "    X_te, Y_te = X_te[:1000], Y_te[:1000]\n",
    "\n",
    "    # 3. Preprocess the input image\n",
    "    X_tr = np.reshape(X_tr, (X_tr.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0],-1))\n",
    "    X_te = np.reshape(X_te, (X_te.shape[0],-1))\n",
    "    \n",
    "    # 4. Normalize the data (subtract the mean image)\n",
    "    mean_img = np.mean(X_tr, axis=0)\n",
    "    X_tr -= mean_img\n",
    "    X_val -= mean_img\n",
    "    X_te -= mean_img\n",
    "    \n",
    "    return X_tr, Y_tr, X_val, Y_val, X_te, Y_te\n",
    "\n",
    "\n",
    "\n",
    "X_tr, Y_tr, X_val, Y_val, X_te, Y_te = get_CIFAR10_data()\n",
    "print 'Train data shape : %s,  Train labels shape : %s' % (X_tr.shape, Y_tr.shape)\n",
    "print 'Validatoin data shape : %s,  Validation labels shape : %s' % (X_val.shape, Y_val.shape)\n",
    "print 'Test data shape : %s,  Test labels shape : %s' % (X_te.shape, Y_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our network we will use SGD with momentum. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplyting it by a decay rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ca3b128ebbdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;31m# Train the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.95\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;31m# Predict on the validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_tr' is not defined"
     ]
    }
   ],
   "source": [
    "input_size = 32*32*3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_tr, Y_tr, X_val, Y_val, 1e-4, 0.95, 0.5, 1000, 200, True)\n",
    "\n",
    "# Predict on the validation set\n",
    "#val_acc = (net.predict(X_val)==Y_val).mean()\n",
    "val_acc = np.mean(net.predict(X_val)==Y_val)\n",
    "\n",
    "print 'Validation accuracy :' , val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the default parameters we provided above, you should get a validation accuracy of about 0.29 on the validation set. This isn't very good.\n",
    "\n",
    "One strategy for getting insight into what's wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.\n",
    "\n",
    "Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-bf4cf6238be4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Plot the loss function and train / validation accuracie\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss_history'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss history'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stats' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAADpCAYAAAATWPImAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x71c8b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss function and train / validation accuracie\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('classification accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt, ceil\n",
    "\n",
    "def visualize_grid(Xs, ubound=255.0, padding=1):\n",
    "    \"\"\"\n",
    "    Reshape a 4D tensor of image data to a grid for easy visualization.\n",
    "    Inputs:\n",
    "    - Xs: Data of shape (N, H, W, C)\n",
    "    - ubound: Output grid will have values scaled to the range [0, ubound]\n",
    "    - padding: The number of blank pixels between elements of the grid\n",
    "    \"\"\"\n",
    "    (N, H, W, C) = Xs.shape\n",
    "    grid_size = int(ceil(sqrt(N)))\n",
    "    grid_height = H * grid_size + padding * (grid_size - 1)\n",
    "    grid_width = W * grid_size + padding * (grid_size - 1)\n",
    "    grid = np.zeros((grid_height, grid_width, C))\n",
    "    next_idx = 0\n",
    "    y0, y1 = 0, H\n",
    "    for y in xrange(grid_size):\n",
    "        x0, x1 = 0, W\n",
    "        for x in xrange(grid_size):\n",
    "            if next_idx < N:\n",
    "                img = Xs[next_idx]\n",
    "                low, high = np.min(img), np.max(img)\n",
    "                grid[y0:y1, x0:x1] = ubound * (img - low) / (high - low)\n",
    "                # grid[y0:y1, x0:x1] = Xs[next_idx]\n",
    "                next_idx += 1\n",
    "            x0 += W + padding\n",
    "            x1 += W + padding\n",
    "        y0 += H + padding\n",
    "        y1 += H + padding\n",
    "    # grid_max = np.max(grid)\n",
    "    # grid_min = np.min(grid)\n",
    "    # grid = ubound * (grid - grid_min) / (grid_max - grid_min)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the weights of the network\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.reshape(32,32,3,-1).transpose(3,0,1,2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "show_net_weights(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the hyperparemeters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, number of training epochs, and regularization strength. You might also consider tuning the learning rate decay but, you should be able to get good performance using the default value.\n",
    "\n",
    "You should be aim to achieve a classification accuracy of grater than 45% on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_net = None # store the best model into this \n",
    "\n",
    "################################################################################################\n",
    "# TODO: Tune hyperparameters using the validation set. Store your best model in best_net.      #\n",
    "#                                                                                              #\n",
    "# To help debug your network, it may help to use visualizations similar to the ones we used    #\n",
    "# above; these visualizations will have significant qualitative differences from the ones      #\n",
    "# we saw above for the poorly tuned network.                                                   #\n",
    "#                                                                                              #\n",
    "# Tweaking hyperparameters by hand can be fun, but you might find it useful to write code      #\n",
    "# to sweep through possible combinations of hyperparameters automatically like we did on the   #\n",
    "# previous exercises.                                                                          #\n",
    "#-------------------------------------WRITE YOUR CODE------------------------------------------#\n",
    "best_val = -1\n",
    "best_stats = None\n",
    "learning_rates = [1e-2, 1e-3]\n",
    "regularization_strengths = [0.4, 0.5, 0.6]\n",
    "results = {} \n",
    "iters = 2000 #100\n",
    "for lr in learning_rates:\n",
    "    for rs in regularization_strengths:\n",
    "        net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "        # Train the network\n",
    "        stats = net.train(X_train, y_train, X_val, y_val,\n",
    "                    num_iters=iters, batch_size=200,\n",
    "                    learning_rate=lr, learning_rate_decay=0.95,\n",
    "                    reg=rs)\n",
    "        \n",
    "        y_train_pred = net.predict(X_train)\n",
    "        acc_train = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = net.predict(X_val)\n",
    "        acc_val = np.mean(y_val == y_val_pred)\n",
    "        \n",
    "        results[(lr, rs)] = (acc_train, acc_val)\n",
    "        \n",
    "        if best_val < acc_val:\n",
    "            best_stats = stats\n",
    "            best_val = acc_val\n",
    "            best_net = net\n",
    "            \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f' % best_val  \n",
    "\n",
    "#------------------------------------END OF YOUR CODE------------------------------------------#\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
